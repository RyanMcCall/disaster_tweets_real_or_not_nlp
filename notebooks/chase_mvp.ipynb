{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweets, Real or Not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "*Think about the problem you are trying to solve as a business case. After you've built your model, who is going to use it and what will they do with it? Who are your customers and stakeholders and what is their need? The need is not the data they are trying to understand -- think of the need as a specific action they would like to take or a question they want to answer.*\n",
    "\n",
    "*What is your model going to **do** in the real world? If you can answer this, and if you keep that answer in mind as you complete your work, you will have a much clearer view of where to go at each step.*\n",
    "\n",
    "* Who are your customers?\n",
    "* What is the problem?\n",
    "* What solution do you propose?\n",
    "* How will you know if your work is good?\n",
    "\n",
    "*Also, think critacally here. If someone was paying you to do this work, what would they want you to build.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Plan\n",
    "\n",
    "*Now that you have a clear idea of what you are trying to accomplish, you need to find a data source. Think of this as working through a maze forwards and backwards at the same time. At the start you have any number of data sets available to work with (and the whole Internet to search and scrape), and at the end is the hypothetical data set that would answer your question immediately if you had it.*\n",
    "\n",
    "* What data, if you had it, would solve your problem right away?\n",
    "* What data do you have access to?\n",
    "* What additional data would be good to have?\n",
    "* What data would be impossible to collect?\n",
    "* What are the best proxies you can find for unavailable or impossible data?\n",
    "* What are the legal or ethical issues you might run into if you were to try to collect all of the types of data you would like to work with?\n",
    "\n",
    "*Of course you need to make a plan to turn your data into the solutions you need. Think of what type of problem this is, what models are commonly used for those types of problems, what types of data those models require and special considerations that may need to be made. Do your homework and find out what approaches other people are using on similar tasks.*\n",
    "\n",
    "* What ML paradigm are you working in? (Classification, Regression, Clustering, etc)\n",
    "* What models are commonly used in this task?\n",
    "* What other solutions are being tried in this field?\n",
    "* What special considerations need to be taken when dealing with these models? (i.e., imbalanced classes, text preprocessing, data leakage, etc)\n",
    "* How will you know that your models work?\n",
    "* How will you recognize and diagnose cases where the predictions are incorrect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "*Keep all your imports in one place. This will make it much easier to see everything that you are using*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sys.path mimic the location of the mvp.ipynb notebook\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Source python scripts\n",
    "import src.acquire\n",
    "import src.prepare\n",
    "import src.explore\n",
    "import src.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire the Data\n",
    "\n",
    "### Data Pipeline\n",
    "\n",
    "*This is where you describe the ETL (Extract, Transform, Load) process. Describe the actions you will need to take to obtain the data from where it is stored, and convert it into a clean format. Also be aware of how the available data is likely to change over time -- how often will it be updated, and how long will the available data be relevant?*\n",
    "\n",
    "*After completing this step, be sure to edit the README data dictionary to include descriptions of where you obtained your data and what information it contains.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your Pipeline Tools\n",
    "\n",
    "*Your work is going to be far less useful if anyone who wants to use it or build on it is going to have to put together a new data set from scratch. Automate everything that you possibly can. (And if you don't think it can be automated, ask your peers and check Google to see what's out there.)*\n",
    "\n",
    "*The pipeline tools will connect to data sources, create local directories to store the data, download the data, clean it, reformat it, and store it. In the following step, \"Run the Pipeline,\" you'll write a function that uses all of these tools to build your data set in one line of code.*\n",
    "\n",
    "*Make sure these steps are reproducible by code. Put some thought into the directory structures and filepaths you are using to save your data, so it's easy to load files you need.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    os.system(\"kaggle competitions download nlp-getting-started -f test.csv -p data/raw/\")\n",
    "    os.system(\"kaggle competitions download nlp-getting-started -f train.csv -p data/raw/\")\n",
    "\n",
    "def data_exists():\n",
    "    return os.path.isfile(\"data/raw/test.csv\") and os.path.isfile(\"data/raw/train.csv\")\n",
    "\n",
    "def run():\n",
    "    print(\"Acquire: downloading raw data files...\")\n",
    "\n",
    "    if data_exists():\n",
    "        print(\"Data already exists\")\n",
    "    else:\n",
    "        download_data()\n",
    "        print(\"Data acquired\")\n",
    "\n",
    "    print(\"Acquire: Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove it Works\n",
    "\n",
    "*Once your tools are built (and are working reliably!!), you'll be able to work much faster if they're out of sight. Collaborators and data scientists who build on your work will also be much more productive if you provide easy access to the data. So do yourself and your team a favor, and package your pipeline tools into a single function that can easily reproduce your working data set. This is what our run function in acquire is for.*\n",
    "\n",
    "*Also a helpful thing to remember is to set this function up so that when you re-run the pipeline, it checks whether the files are already available locally. This will save you lots of time downloading large files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquire: downloading raw data files...\n",
      "Data acquired\n",
      "Acquire: Completed!\n"
     ]
    }
   ],
   "source": [
    "src.acquire.run()\n",
    "df = pd.read_csv('data/raw/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "*This is the important phase and it is seen as a chore by some, even though I personally enjoy the puzzle that this stage always provides. If you tend to think of data scrubbing as a drag on your workflow, it's well worth your time to explore some new pandas functions you're unfamiliar with and speed up your workflow. Try to learn one or two new tricks with each project you work on.*\n",
    "\n",
    "*The primary scientific component to scrubbing data is going to be the thought process around deciding what to do with missing data. There are frequently no right or wrong answers for what to do with missing values, so be sure to record your thoughts, and share your process with peers or collaborators to get a sanity check.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load (and Split) the Data\n",
    "\n",
    "*You may already have your data loaded in this notebook after the Acquire stage has been run. But you probably don't want to have to re-run your data gathering pipeline every time you try a new idea for your model. Write some code here to load your cached data set so you won't have to start from scratch.*\n",
    "\n",
    "*It is especially important to go back to a clean data set with each iteration that you want to test or add to your model. Your new results will not be reproducible if this iteration is developed on data that has been transformed in any way during the previous iteration.*\n",
    "\n",
    "*Lastly if possible split the data at this stage. It is always best to split the data as early as possible so that you are making you decision of preparation on a subset of the whole dataset as the model you are building will be doing on future unseen data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "harm                     41\n",
       "sinking                  41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                             104\n",
       "New York                         71\n",
       "United States                    50\n",
       "London                           45\n",
       "Canada                           29\n",
       "                               ... \n",
       "probably not home                 1\n",
       "ChicagoRObotz                     1\n",
       "Rockland County, NY               1\n",
       "westwestwestwestwestwestwest      1\n",
       "Lincoln, IL                       1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to fill all the NA data for the keyword and location columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keyword.fillna('no_keyword', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location.fillna('no_location', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>no_keyword</td>\n",
       "      <td>no_location</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     keyword     location  \\\n",
       "0   1  no_keyword  no_location   \n",
       "1   4  no_keyword  no_location   \n",
       "2   5  no_keyword  no_location   \n",
       "3   6  no_keyword  no_location   \n",
       "4   7  no_keyword  no_location   \n",
       "\n",
       "                                                text  target  \n",
       "0  Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1             Forest fire near La Ronge Sask. Canada       1  \n",
       "2  All residents asked to 'shelter in place' are ...       1  \n",
       "3  13,000 people receive #wildfires evacuation or...       1  \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "keyword     0\n",
       "location    0\n",
       "text        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "\n",
    "*Make sure that your data is in a format that your models can work with, and fix any problems that need attention. This may include formatting strings, removing duplicate records, and handling missing data.*\n",
    "\n",
    "*There are multiple approaches to handling missing records, including removing them, inferring the values either statistically or from other insights, or finding other data sources to fill the gaps. Be sure to note down your thought process for whatever decision you make.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write functions that prepare the data from your data/raw and store\n",
    "# the prepared data in data/\n",
    "# Once that is done move the functions to prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "**If you haven't split the data yet, do it before you explore**\n",
    "\n",
    "*Before you start exploring the data, write out your thought process about what you're looking for and what you expect to find. Take a minute to confirm that your plan actually makes sense.*\n",
    "\n",
    "*Calculate summary statistics and plot some charts to give you an idea what types of useful relationships might be in your dataset. Use these insights to go back and download additional data or engineer new features if necessary. Not now though... remember we're still just trying to finish the MVP!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Distributions\n",
    "\n",
    "*It's good to have a sense of things like the scale and the spread of the data you're working with. But looking at the distribution of each feature can be a helpful way to determine if more features are needed.*\n",
    "\n",
    "*Look for outliers. Take a closer look at the observations that stand out from the rest. Is there anything about these data points that might explain why they aren't behaving like the others? If so, that's a feature you would want to add to your model, to control for such a big difference.*\n",
    "\n",
    "*Look for clusters in your histograms and scatterplots of various features. If the variance is inconsistent, with dense clusters of observations within the feature space, try to figure out what makes those observations so much more similar than the others. This could point to another feature to add to your data. Investigate any unusual patterns that look like a feature is not being drawn from a random distribution.*\n",
    "\n",
    "*Look for multiple modes within the distributions of your features. This is a sign that there are categorical variables within your data set. Try to find out what these classes are, if you don't already know, and label your observations if possible.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Relationships\n",
    "\n",
    "*The \"eyeball test\" is the quickest way to check that your hypothesis is on the right track. Plot your inputs against your outputs. If there is real predictive power for the model you are trying to build, you should be able to make reasonable guesses (just by pointing at the chart, no need for precise values yet) of what the output should be for any given input value.*\n",
    "\n",
    "*In a regression model, we are looking for clear relationships between our features and targets. Generate pairplots and look for features that have high correlations with the target variable.*\n",
    "\n",
    "*In a classification model, we are looking for features that separate the population into distinct distributions. Generate pairplots that are color-coded by your categories and look for features where the categories have distinct distributions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the Data\n",
    "\n",
    "*Describe the algorithms that you are considering. How do they work? Why are they good choices for this data and problem space?*\n",
    "\n",
    "*What nuances in the data will you have to be aware of in order to avoid introducing bias to your model? What steps will you need to take to prevent overfitting? What risks are there for data leakage?*\n",
    "\n",
    "### Train Validation Test Split\n",
    "\n",
    "*Pay special attention here to what data is going into your training and test sets. Is there any data leakage? Make sure that you are testing your model on information it has not seen during the learning process. If this is a classification problem, make sure that your classes are reasonably balanced.*\n",
    "\n",
    "*Also, keep in mind that after you've trained and evaluated your model, you may want to look at the results of specific missed predictions. Set up your training and test data sets so that it will be easy to identify the record id when you are looking at a given prediction result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "*These are standard processing steps to prepare your data for modeling. But remember that any code you use to scale or encode your data has to come from the observations in your training set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Model\n",
    "\n",
    "*Write down any thoughts you may have about working with these algorithms on this data. What looks to have been the most successful design choices? What pain points are you running into? What other ideas do you want to try out as you iterate on this pipeline?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and Score\n",
    "\n",
    "Important: Stick to the metrics you chose to test before you trained and evaluated your model! Remember that this is the standard you selected before your analysis as the one that you would find most convincing that the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the Model\n",
    "\n",
    "Write up the things you learned, and how well your model performed. Be sure address the model's strengths and weaknesses. What types of data does it handle well? What types of observations tend to give it a hard time? What future work would you or someone reading this might want to do, building on the lessons learned and tools developed in this project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Metrics in Context\n",
    "How did the model perform on the key metrics you chose to demonstrate its usefulness? What counts as good or bad performance? How well do humans perform on this task? How well would you expect to do with random dice rolls? What are the costs associated with missed predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Errors\n",
    "\n",
    "Look at some of the observations with missed predictions. (Do this in your validation set, never look at individual records in your test set!) Are there common patterns among the observations with bad results? Do you have data that you can include in your model that will capture these patterns or is this a task that will need another research project to solve?\n",
    "\n",
    "This is your last step in the iteration cycle; if you can't find anything else you can work on here with your present data, project scope, and deadlines, then it's time to wrap things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strengths and Weaknesses\n",
    "\n",
    "Once you've gone through your iteration cycles and are finished with this version of the model, or this particular project, provide an assessment of what types of observations are handled well by your model, and what circumstances seem to give it trouble. This will point you and others towards more questions for future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: What Can We Do Now?\n",
    "\n",
    "Reporting a model's results is good, and is the main objective of any data science project. But a project is one thing, a career is another. A question is one thing, but science is another. If you've carried out your research with a mindset of curiosity and creativity, then by now you should have plenty more, and much better informed, questions about this topic than what you started with.\n",
    "\n",
    "So in addition to reporting on the question you investigated and the answers you found, think of the needs of your team, your users, and your peers in the industry, and make some recommendations that answer these two questions:\n",
    "\n",
    "What are some unanswered questions in my project where more information (additional data sources, deeper understanding, other models or tools) might help improve these results?\n",
    "What are other needs or problems where my model or my approach may be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "import folium\n",
    "\n",
    "\n",
    "# inspired by https://alysivji.github.io/getting-started-with-folium.html\n",
    "def map_points(df, lat_col='latitude', lon_col='longitude', zoom_start=11, \\\n",
    "                plot_points=False, pt_radius=15, \\\n",
    "                draw_heatmap=False, heat_map_weights_col=None, \\\n",
    "                heat_map_weights_normalize=True, heat_map_radius=15):\n",
    "    \"\"\"Creates a map given a dataframe of points. Can also produce a heatmap overlay\n",
    "\n",
    "    Arg:\n",
    "        df: dataframe containing points to maps\n",
    "        lat_col: Column containing latitude (string)\n",
    "        lon_col: Column containing longitude (string)\n",
    "        zoom_start: Integer representing the initial zoom of the map\n",
    "        plot_points: Add points to map (boolean)\n",
    "        pt_radius: Size of each point\n",
    "        draw_heatmap: Add heatmap to map (boolean)\n",
    "        heat_map_weights_col: Column containing heatmap weights\n",
    "        heat_map_weights_normalize: Normalize heatmap weights (boolean)\n",
    "        heat_map_radius: Size of heatmap point\n",
    "\n",
    "    Returns:\n",
    "        folium map object\n",
    "    \"\"\"\n",
    "\n",
    "    ## center map in the middle of points center in\n",
    "    middle_lat = df[lat_col].median()\n",
    "    middle_lon = df[lon_col].median()\n",
    "\n",
    "    curr_map = folium.Map(location=[middle_lat, middle_lon],\n",
    "                          zoom_start=zoom_start)\n",
    "\n",
    "    # add points to map\n",
    "    if plot_points:\n",
    "        for _, row in df.iterrows():\n",
    "            folium.CircleMarker([row[lat_col], row[lon_col]],\n",
    "                                radius=pt_radius,\n",
    "                                popup=row['location'],\n",
    "                                fill_color=\"#3db7e4\", # divvy color\n",
    "                               ).add_to(curr_map)\n",
    "\n",
    "    # add heatmap\n",
    "    if draw_heatmap:\n",
    "        # convert to (n, 2) or (n, 3) matrix format\n",
    "        if heat_map_weights_col is None:\n",
    "            cols_to_pull = [lat_col, lon_col]\n",
    "        else:\n",
    "            # if we have to normalize\n",
    "            if heat_map_weights_normalize:\n",
    "                df[heat_map_weights_col] = \\\n",
    "                    df[heat_map_weights_col] / df[heat_map_weights_col].sum()\n",
    "\n",
    "            cols_to_pull = [lat_col, lon_col, heat_map_weights_col]\n",
    "\n",
    "        stations = df[cols_to_pull].values\n",
    "        curr_map.add_child(plugins.HeatMap(stations, radius=heat_map_radius))\n",
    "\n",
    "    return curr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'lat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9ebddcdda829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     df=df[\n\u001b[1;32m      3\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     ].sample(500, random_state=911) , # for memory constrains\n\u001b[1;32m      6\u001b[0m     \u001b[0mlat_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'lat'"
     ]
    }
   ],
   "source": [
    "map_points(\n",
    "    df=df[\n",
    "        df \n",
    "        & ~df.lat.isnull()\n",
    "    ].sample(500, random_state=911) , # for memory constrains\n",
    "    lat_col='lat',\n",
    "    lon_col='lon', \n",
    "    zoom_start=2, # change to `1` to set global world view\n",
    "    plot_points=False, \n",
    "    pt_radius=0.75,\n",
    "    draw_heatmap=True, \n",
    "    heat_map_weights_col='target',\n",
    "    heat_map_weights_normalize=True,\n",
    "    heat_map_radius=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
